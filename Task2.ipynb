{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8kHkJ7M_Qz2L"
      },
      "outputs": [],
      "source": [
        "# Import all modules.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working on zip file.\n",
        "# My idea is to create folder from zip file as to easy in use.\n",
        "\n",
        "import zipfile\n",
        "\n",
        "zip_file= \"/content/New assi.zip\"\n",
        "\n",
        "#  extraction directory\n",
        "extract_dir = \"my_images\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "3coVSO-HTFVW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar for Masks.\n",
        "import zipfile\n",
        "\n",
        "# Specify the name of the uploaded zip file\n",
        "zip_file= \"/content/Q2.zip\"\n",
        "\n",
        "# Specify the extraction directory\n",
        "extract_dir = \"my_zip_images\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n"
      ],
      "metadata": {
        "id": "XyHgNHE9TNhU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your images folder\n",
        "image_folder = \"/content/my_images/images\"\n",
        "mask_folder= \"/content/my_zip_images/masks\""
      ],
      "metadata": {
        "id": "iLSl1s4_Tbz9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7BMPVWVeThLD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess image.\n",
        "def preprocess_data(image_folder,mask_folder):\n",
        "  image=[]\n",
        "  mask_=[]\n",
        "\n",
        "  image_name=os.listdir(image_folder)\n",
        "  mask_name=os.listdir(mask_folder)   # list of filenames\n",
        "\n",
        "  mask_name.sort()\n",
        "  image_name.sort()  # To match image and mask name\n",
        "\n",
        "  for image_name,mask_name in tqdm(zip(image_name,mask_name),total=len(image_name)):\n",
        "\n",
        "    image_p=os.path.join(image_folder,image_name)\n",
        "    img=cv2.imread(image_p)\n",
        "    img=cv2.resize(img,(800,540))\n",
        "\n",
        "    img=img.astype('float32')/255.0\n",
        "\n",
        "    # Similarly for mask image.\n",
        "    mask_p=os.path.join(mask_folder,mask_name)\n",
        "    mask=cv2.imread(mask_p,cv2.IMREAD_GRAYSCALE)\n",
        "    mask=cv2.resize(mask,(800,540))\n",
        "\n",
        "    mask=mask.astype('float32')/255.0\n",
        "    mask=np.where(mask>0,1,0)\n",
        "\n",
        "    mask_.append(mask)\n",
        "    image.append(img)\n",
        "  return np.array(image), np.array(mask_)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGey-4wOTk8o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it's time to load data.\n",
        "imgs,masks=preprocess_data(image_folder,mask_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GBuqVvJU1L5",
        "outputId": "99a4849f-45aa-4cad-ac74-31256353bb66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 317/317 [00:03<00:00, 82.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYuhSMxaYxN5",
        "outputId": "15ea111d-a41a-4f65-93d5-77166e2f6926"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(317, 540, 800, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtOAGkATY1Je",
        "outputId": "5002237b-e1de-4130-9c57-fd9b4bd8df3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(317, 540, 800)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now its time for splitting.\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs, masks, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "haietkNXZBhl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check nonzero pixcels in image.\n",
        "np.nonzero(X_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPfDB7H9cs2z",
        "outputId": "6d07acab-e7e0-4e00-8460-acd82c1a880c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  1,   1,   1, ..., 538, 538, 538]),\n",
              " array([ 99,  99,  99, ..., 784, 784, 784]),\n",
              " array([0, 1, 2, ..., 0, 1, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now time for model construction.\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input, Conv2D, concatenate,UpSampling2D,MaxPooling2D\n"
      ],
      "metadata": {
        "id": "iyr4PQigc3I-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I tried U-Net model especially for segmentation.\n",
        "# here I refer model Artitecture from Kaggle,Chatgpt.\n",
        "\n",
        "def create_unet_model(inp_shape):\n",
        "    inputs = Input(inp_shape)\n",
        "    # Here i need to define Encoder Artitecture and for decoder Artit\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(62, 3, activation='relu', padding='same')(inputs)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "    conv2 = Conv2D(126, 3, activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "    # Decoder\n",
        "    up1 = UpSampling2D()(pool2)\n",
        "    concat1 = concatenate([conv2, up1], axis=-1)\n",
        "    conv3 = Conv2D(64, 3, activation='relu', padding='same')(concat1)\n",
        "    up2 = UpSampling2D()(conv3)\n",
        "    concat2 = concatenate([conv1, up2], axis=-1)\n",
        "    segmentation_output = Conv2D(1, 1, activation='sigmoid')(concat2)\n",
        "\n",
        "    model = Model(inputs, segmentation_output)\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qdyDDl0UdB3Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: My computational power + collab gpu memory exceeds so i didn't train for more epochs"
      ],
      "metadata": {
        "id": "gkj4RYiUeX_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Limit GPU memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "ntRd9vE9hnJb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable mixed precision training\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "D8fCARTIhpDm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Its time for Training.\n",
        "input_shape = X_train[0].shape\n",
        "seg_model = create_unet_model(input_shape)\n",
        "seg_model.fit(X_train, y_train, epochs=1, batch_size=8, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r4szfSpeJLm",
        "outputId": "d59af02b-2f32-4508-e7bf-bb4137291058"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 [==============================] - 438s 6s/step - loss: 0.2277 - accuracy: 0.9619 - val_loss: 0.0946 - val_accuracy: 0.9938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79b5588af310>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights\n",
        "weights_path = '/content/my_images.h5'\n",
        "seg_model.save_weights(weights_path)"
      ],
      "metadata": {
        "id": "9qFd0wzI2Tmu"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}